ldm:
  base_lr: 0.000025
  params:
    spatial_dims: 2
    in_channels: 5 # 4, 5
    out_channels: 4
    num_res_blocks: 2
    num_channels: [256, 256, 512]
    attention_levels: [False, False, True]
    with_conditioning: True
    cross_attention_dim: 32768 # 512 pras masks, 1024 pra text
    num_head_channels: [0, 0, 512] # 0,0,512
    num_class_embeds: 1000 # 2
  scheduler:
    schedule: "linear_beta" # scaled_linear_beta
    num_train_timesteps: 1000
    beta_start: 0.0015
    beta_end: 0.0195  # 0205
